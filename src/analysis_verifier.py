# AIåˆ†æç»“è®ºéªŒè¯å’Œä¿®æ­£æ¨¡å—

import re
import json
import requests
import random
from typing import Dict, List, Tuple, Optional, Any
from config import MODEL_CONFIG, VERIFICATION_CONFIG
from analysis_data_text_order import parse_and_sort_data, parse_structured_data_for_stats, calculate_statistics


class AnalysisVerifier:
    """AIåˆ†æç»“è®ºéªŒè¯å’Œä¿®æ­£å™¨"""
    
    def __init__(self):
        self.base_url = MODEL_CONFIG['base_url']
        self.model_name = MODEL_CONFIG['llm_model']
        self.api_key = MODEL_CONFIG.get('api_key', 'sk-XIval4xD5HWrvG7956C534B6Cd7348C2B22dFc22B1Ca308e')
        self.timeout = MODEL_CONFIG.get('timeout', 120)
    
    def verify_and_correct_analysis(self, 
                                  original_analysis: str, 
                                  user_input: str, 
                                  chart_obj: str, 
                                  task_info: dict) -> Tuple[str, List[str]]:
        """
        éªŒè¯å¹¶ä¿®æ­£AIåˆ†æç»“è®º
        
        Args:
            original_analysis: åŸå§‹åˆ†æç»“è®º
            user_input: ç”¨æˆ·è¾“å…¥é—®é¢˜
            chart_obj: åŸå§‹æ•°æ®
            task_info: ä»»åŠ¡ä¿¡æ¯
            
        Returns:
            Tuple[ä¿®æ­£åçš„åˆ†æç»“è®º, å‘ç°çš„é—®é¢˜åˆ—è¡¨]
        """
        print("ğŸ” å¼€å§‹éªŒè¯åˆ†æç»“è®º...")
        
        # 1. è§£æåŸå§‹æ•°æ®
        structured_data, data_headers = parse_structured_data_for_stats(chart_obj)
        sort_results = parse_and_sort_data(chart_obj)
        statistics = calculate_statistics(structured_data, data_headers)
        
        # 2. æ£€æµ‹æ•°æ®ç±»å‹
        data_type = self._detect_data_type(structured_data, data_headers)
        print(f"ğŸ“Š æ£€æµ‹åˆ°æ•°æ®ç±»å‹: {data_type}")
        
        # 3. æ‰§è¡Œäº‹å®æ£€æŸ¥
        fact_check_results = self._perform_fact_check(
            original_analysis, structured_data, data_headers, statistics, sort_results, data_type
        )
        
        # 4. å¦‚æœå‘ç°é—®é¢˜ï¼Œè¿›è¡Œä¿®æ­£
        if fact_check_results['has_errors']:
            print(f"âš ï¸ å‘ç° {len(fact_check_results['errors'])} ä¸ªé—®é¢˜ï¼Œå¼€å§‹ä¿®æ­£...")
            try:
                corrected_analysis = self._correct_analysis(
                    original_analysis, user_input, chart_obj, sort_results, 
                    statistics, data_type, fact_check_results['errors']
                )
                return corrected_analysis, fact_check_results['errors']
            except Exception as e:
                print(f"âŒ ä¿®æ­£è¿‡ç¨‹å¤±è´¥: {e}")
                print("ğŸ“ è¿”å›åŸå§‹åˆ†æç»“æœï¼Œä½†ä¼šè®°å½•å‘ç°çš„é—®é¢˜")
                return original_analysis, fact_check_results['errors']
        else:
            print("âœ… åˆ†æç»“è®ºéªŒè¯é€šè¿‡ï¼Œæ— éœ€ä¿®æ­£")
            return original_analysis, []
    
    def _detect_data_type(self, structured_data: List[Dict], headers: List[str]) -> str:
        """æ£€æµ‹æ•°æ®ç±»å‹ï¼šæ—¶é—´åºåˆ— vs åœ°å¸‚æ•°æ®"""
        if not headers:
            return "unknown"
        
        # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦ä¸ºæœˆä»½
        if 'æœˆä»½' in headers[0]:
            return "time_series"
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åœ°å¸‚åç§°
        if 'åœ°å¸‚' in headers[0] or any('åœ°å¸‚' in str(headers)):
            return "city_data"
        
        # æ£€æŸ¥æ•°æ®å†…å®¹
        if structured_data:
            first_col_values = [str(item.get(headers[0], '')) for item in structured_data]
            # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´æ ¼å¼ï¼ˆå¦‚202501ï¼‰
            time_pattern = re.compile(r'^\d{6}$')  # YYYYMMæ ¼å¼
            if any(time_pattern.match(val) for val in first_col_values):
                return "time_series"
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åœ°å¸‚åç§°
            city_names = ['æ²ˆé˜³', 'å¤§è¿', 'éå±±', 'æŠšé¡º', 'æœ¬æºª', 'ä¸¹ä¸œ', 'é”¦å·', 'è¥å£', 
                         'é˜œæ–°', 'è¾½é˜³', 'ç›˜é”¦', 'é“å²­', 'æœé˜³', 'è‘«èŠ¦å²›']
            if any(city in val for val in first_col_values for city in city_names):
                return "city_data"
        
        return "unknown"
    
    def _perform_fact_check(self, analysis: str, structured_data: List[Dict], 
                          headers: List[str], statistics: Dict, sort_results: str, 
                          data_type: str) -> Dict[str, Any]:
        """æ‰§è¡Œäº‹å®æ£€æŸ¥"""
        errors = []
        
        # 1. æ£€æŸ¥æ•°æ®ç±»å‹ä¸åŒ¹é…
        if data_type == "time_series":
            city_names = ['æ²ˆé˜³', 'å¤§è¿', 'éå±±', 'æŠšé¡º', 'æœ¬æºª', 'ä¸¹ä¸œ', 'é”¦å·', 'è¥å£', 
                         'é˜œæ–°', 'è¾½é˜³', 'ç›˜é”¦', 'é“å²­', 'æœé˜³', 'è‘«èŠ¦å²›']
            mentioned_cities = [city for city in city_names if city in analysis]
            if mentioned_cities:
                errors.append({
                    "type": "data_type_mismatch",
                    "description": f"æ—¶é—´åºåˆ—æ•°æ®ä¸­ä¸åº”åŒ…å«åœ°å¸‚åˆ†æï¼Œä½†æåˆ°äº†: {', '.join(mentioned_cities)}",
                    "severity": "high"
                })
        
        elif data_type == "city_data":
            # æ£€æŸ¥æ—¶é—´åºåˆ—ç›¸å…³çš„é”™è¯¯è¡¨è¿°
            time_keywords = ['æ—¶é—´ç‚¹', 'æœˆä»½å˜åŒ–', 'ç¯æ¯”', 'æ—¶é—´åºåˆ—']
            mentioned_time_analysis = [kw for kw in time_keywords if kw in analysis]
            if mentioned_time_analysis and not any(month in analysis for month in ['202501', '202502', '202503', '202504', '202505', '202506', '202507', '202508']):
                errors.append({
                    "type": "inappropriate_time_analysis",
                    "description": f"åœ°å¸‚æ•°æ®ä¸­ä½¿ç”¨äº†ä¸å½“çš„æ—¶é—´åºåˆ—åˆ†æ: {', '.join(mentioned_time_analysis)}",
                    "severity": "medium"
                })
        
        # 2. æ£€æŸ¥æ•°å€¼å‡†ç¡®æ€§
        self._check_numerical_accuracy(analysis, structured_data, headers, statistics, errors)
        
        # 3. æ£€æŸ¥æ’åºå‡†ç¡®æ€§
        self._check_ranking_accuracy(analysis, sort_results, errors)
        
        # 4. æ£€æŸ¥é€»è¾‘ä¸€è‡´æ€§
        self._check_logical_consistency(analysis, errors)
        
        return {
            "has_errors": len(errors) > 0,
            "errors": errors,
            "error_count": len(errors)
        }
    
    def _check_numerical_accuracy(self, analysis: str, structured_data: List[Dict], 
                                headers: List[str], statistics: Dict, errors: List[Dict]):
        """æ£€æŸ¥æ•°å€¼å‡†ç¡®æ€§"""
        # æå–åˆ†æä¸­çš„æ•°å€¼
        number_pattern = re.compile(r'(\d+\.?\d*)%?')
        mentioned_numbers = number_pattern.findall(analysis)
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ä¸å­˜åœ¨çš„æ•°å€¼
        actual_numbers = set()
        for item in structured_data:
            for header in headers[1:]:  # è·³è¿‡ç¬¬ä¸€åˆ—
                value = item.get(header)
                if isinstance(value, (int, float)):
                    actual_numbers.add(str(value))
                    actual_numbers.add(f"{value:.1f}")
                    actual_numbers.add(f"{value:.2f}")
        
        # æ·»åŠ ç»Ÿè®¡æ•°æ®ä¸­çš„æ•°å€¼
        for indicator, stats in statistics.items():
            actual_numbers.add(str(stats['mean']))
            actual_numbers.add(f"{stats['mean']:.1f}")
            actual_numbers.add(str(stats['max']))
            actual_numbers.add(str(stats['min']))
        
        # æ£€æŸ¥åˆ†æä¸­æ˜¯å¦æœ‰ç¼–é€ çš„æ•°å€¼ï¼ˆç®€å•æ£€æŸ¥ï¼‰
        suspicious_numbers = []
        for num in mentioned_numbers:
            if num not in actual_numbers and float(num) > 0:
                # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦æ˜¯åˆç†çš„è®¡ç®—ç»“æœ
                num_val = float(num)
                if num_val > 200 or (num_val > 100 and '%' in analysis):  # å¯ç–‘çš„å¤§æ•°å€¼
                    suspicious_numbers.append(num)
        
        if suspicious_numbers:
            errors.append({
                "type": "suspicious_numbers",
                "description": f"å¯èƒ½ç¼–é€ çš„æ•°å€¼: {', '.join(suspicious_numbers)}",
                "severity": "medium"
            })
    
    def _check_ranking_accuracy(self, analysis: str, sort_results: str, errors: List[Dict]):
        """æ£€æŸ¥æ’åºå’Œæ’åå‡†ç¡®æ€§"""
        # æå–æ’åºç»“æœä¸­çš„å®é™…æ’å
        ranking_info = {}
        lines = sort_results.split('\n')
        
        for line in lines:
            if 'ä»é«˜åˆ°ä½ä¾æ¬¡ä¸º' in line or 'æ—¶é—´åºåˆ—ä¸º' in line:
                # æå–æŒ‡æ ‡åç§°
                if 'ä»é«˜åˆ°ä½ä¾æ¬¡ä¸º' in line:
                    indicator = line.split('ä»é«˜åˆ°ä½ä¾æ¬¡ä¸º')[0]
                    ranking_part = line.split('ä»é«˜åˆ°ä½ä¾æ¬¡ä¸º')[1]
                    # æå–åœ°å¸‚æ’åº
                    city_pattern = re.compile(r'([^ï¼Œ,]+?)(\d+\.?\d*)([%ä¸‡å…ƒæˆ·]*)(?:[ï¼ˆ(][^ï¼‰)]*[ï¼‰)])?')
                    matches = city_pattern.findall(ranking_part)
                    if matches:
                        ranking_info[indicator] = [(match[0].strip(), float(match[1])) for match in matches]
        
        # æ£€æŸ¥åˆ†æä¸­çš„"æœ€é«˜"ã€"æœ€ä½"ã€"ç¬¬ä¸€"ç­‰è¡¨è¿°æ˜¯å¦å‡†ç¡®
        superlative_pattern = re.compile(r'([^ï¼Œã€‚ï¼ï¼Ÿ]*?)([æœ€ç¬¬])([é«˜ä½ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)([^ï¼Œã€‚ï¼ï¼Ÿ]*?)([æ˜¯ä¸º]?)([^ï¼Œã€‚ï¼ï¼Ÿ]*?)([(\d+\.?\d*)%?ä¸‡å…ƒæˆ·]?)')
        matches = superlative_pattern.findall(analysis)
        
        for match in matches:
            # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥å®ç°å…·ä½“çš„æ’åéªŒè¯é€»è¾‘
            # ç”±äºå¤æ‚æ€§ï¼Œæš‚æ—¶æ ‡è®°ä¸ºéœ€è¦äººå·¥æ£€æŸ¥
            pass
    
    def _check_logical_consistency(self, analysis: str, errors: List[Dict]):
        """æ£€æŸ¥é€»è¾‘ä¸€è‡´æ€§"""
        # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªç›¸çŸ›ç›¾çš„è¡¨è¿°
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', analysis)
        
        # æ£€æŸ¥åŒä¸€å®ä½“çš„çŸ›ç›¾æè¿°
        entities = {}
        for sentence in sentences:
            # æå–åœ°å¸‚åç§°å’Œç›¸å…³æè¿°
            city_names = ['æ²ˆé˜³', 'å¤§è¿', 'éå±±', 'æŠšé¡º', 'æœ¬æºª', 'ä¸¹ä¸œ', 'é”¦å·', 'è¥å£', 
                         'é˜œæ–°', 'è¾½é˜³', 'ç›˜é”¦', 'é“å²­', 'æœé˜³', 'è‘«èŠ¦å²›']
            for city in city_names:
                if city in sentence:
                    if city not in entities:
                        entities[city] = []
                    entities[city].append(sentence.strip())
        
        # æ£€æŸ¥çŸ›ç›¾ï¼ˆç®€å•æ£€æŸ¥ï¼šåŒä¸€åœ°å¸‚æ—¢æ˜¯æœ€é«˜åˆæ˜¯æœ€ä½ï¼‰
        for city, descriptions in entities.items():
            high_count = sum(1 for desc in descriptions if 'æœ€é«˜' in desc or 'ç¬¬ä¸€' in desc)
            low_count = sum(1 for desc in descriptions if 'æœ€ä½' in desc or 'å«åº•' in desc)
            
            if high_count > 0 and low_count > 0:
                errors.append({
                    "type": "logical_contradiction",
                    "description": f"{city}åŒæ—¶è¢«æè¿°ä¸ºæœ€é«˜å’Œæœ€ä½",
                    "severity": "high"
                })
    
    def _correct_analysis(self, original_analysis: str, user_input: str, chart_obj: str,
                         sort_results: str, statistics: Dict, data_type: str, 
                         errors: List[Dict]) -> str:
        """ä½¿ç”¨å¤§æ¨¡å‹ä¿®æ­£åˆ†æç»“è®º"""
        
        # æ„å»ºé”™è¯¯æè¿°
        error_descriptions = []
        for error in errors:
            error_descriptions.append(f"- {error['description']} (ä¸¥é‡ç¨‹åº¦: {error['severity']})")
        
        errors_text = '\n'.join(error_descriptions)
        
        # æ ¹æ®æ•°æ®ç±»å‹æ„å»ºä¸åŒçš„ä¿®æ­£æç¤ºè¯
        if data_type == "time_series":
            correction_prompt = self._build_time_series_correction_prompt(
                original_analysis, user_input, chart_obj, sort_results, statistics, errors_text
            )
        elif data_type == "city_data":
            correction_prompt = self._build_city_data_correction_prompt(
                original_analysis, user_input, chart_obj, sort_results, statistics, errors_text
            )
        else:
            correction_prompt = self._build_general_correction_prompt(
                original_analysis, user_input, chart_obj, sort_results, statistics, errors_text
            )
        
        # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œä¿®æ­£
        return self._call_llm_for_correction(correction_prompt)
    
    def _build_time_series_correction_prompt(self, original_analysis: str, user_input: str, 
                                           chart_obj: str, sort_results: str, statistics: Dict, 
                                           errors_text: str) -> str:
        """æ„å»ºæ—¶é—´åºåˆ—æ•°æ®çš„ä¿®æ­£æç¤ºè¯"""
        stats_summary = self._build_statistics_summary(statistics)
        
        return f"""ä½ æ˜¯ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œéœ€è¦ä¿®æ­£ä¸€ä»½æœ‰é—®é¢˜çš„æ—¶é—´åºåˆ—åˆ†ææŠ¥å‘Šã€‚

ã€åŸå§‹åˆ†ææŠ¥å‘Šã€‘ï¼ˆå­˜åœ¨é”™è¯¯ï¼Œéœ€è¦ä¿®æ­£ï¼‰ï¼š
{original_analysis}

ã€å‘ç°çš„é—®é¢˜ã€‘ï¼š
{errors_text}

ã€åŸå§‹æ•°æ®ç±»å‹ã€‘ï¼šæ—¶é—´åºåˆ—æ•°æ®
ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{user_input}
ã€åŸå§‹æ•°æ®ã€‘ï¼š{chart_obj}

ã€å‡†ç¡®çš„æ’åºç»“æœã€‘ï¼š
{sort_results}

ã€å‡†ç¡®ç»Ÿè®¡ä¿¡æ¯ã€‘ï¼š
{stats_summary}

ã€ä¿®æ­£è¦æ±‚ã€‘ï¼š
1. **ä¸¥æ ¼ç¦æ­¢**ï¼šç»å¯¹ä¸èƒ½æåŠä»»ä½•åœ°å¸‚åç§°ï¼ˆå¦‚æ²ˆé˜³ã€å¤§è¿ã€é”¦å·ç­‰ï¼‰ï¼Œè¿™æ˜¯æ—¶é—´åºåˆ—æ•°æ®
2. **é‡ç‚¹åˆ†æ**ï¼šä¸“æ³¨äºæ—¶é—´è¶‹åŠ¿ã€ç¯æ¯”å˜åŒ–ã€å­£èŠ‚æ€§ç‰¹å¾ã€è½¬æŠ˜ç‚¹ç­‰æ—¶é—´ç»´åº¦çš„åˆ†æ
3. **æ•°æ®å‡†ç¡®æ€§**ï¼šæ‰€æœ‰æ•°å€¼å¿…é¡»æ¥æºäºæ’åºç»“æœæˆ–ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸¥ç¦ç¼–é€ 
4. **é€»è¾‘ä¸€è‡´æ€§**ï¼šç¡®ä¿å‰åè¡¨è¿°ä¸çŸ›ç›¾
5. **åˆ†ææ·±åº¦**ï¼šåˆ†æè¶‹åŠ¿å˜åŒ–çš„åŸå› å’Œä¸šåŠ¡å«ä¹‰
6. **æ ¼å¼è¦æ±‚**ï¼šä¿æŒç®€æ´ä¸“ä¸šï¼Œé‡ç‚¹çªå‡ºï¼Œä¸è¶…è¿‡300å­—

ã€æ—¶é—´åºåˆ—åˆ†ææ ·ä¾‹ã€‘ï¼š
- ç»¼åˆä¿æœ‰ç‡å‘ˆæ³¢åŠ¨æ€åŠ¿ï¼š8ä¸ªæœˆä¸­æœ‰4ä¸ªæœˆä½äºå‡å€¼ï¼Œéœ€å…³æ³¨é˜¶æ®µæ€§ä¸‹é™é£é™©
- è§„æ¨¡ä¿æœ‰ç‡æŒç»­ä¸‹æ»‘ï¼šä»202501çš„100.0%é™è‡³202508çš„97.9%ï¼Œå‘ˆç°æ˜æ˜¾ä¸‹é™è¶‹åŠ¿
- ä»·å€¼ä¿æœ‰ç‡æ³¢åŠ¨è¾ƒå¤§ï¼š202507è¾¾åˆ°å³°å€¼99.77%ï¼Œä½†202508å›è½è‡³95.8%ï¼Œæ³¢åŠ¨å¹…åº¦è¾¾3.97ä¸ªç™¾åˆ†ç‚¹

è¯·åŸºäºä¸Šè¿°è¦æ±‚ï¼Œé‡æ–°ç”Ÿæˆå‡†ç¡®çš„æ—¶é—´åºåˆ—åˆ†ææŠ¥å‘Šï¼š"""
    
    def _build_city_data_correction_prompt(self, original_analysis: str, user_input: str, 
                                         chart_obj: str, sort_results: str, statistics: Dict, 
                                         errors_text: str) -> str:
        """æ„å»ºåœ°å¸‚æ•°æ®çš„ä¿®æ­£æç¤ºè¯"""
        stats_summary = self._build_statistics_summary(statistics)
        
        return f"""ä½ æ˜¯ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œéœ€è¦ä¿®æ­£ä¸€ä»½æœ‰é—®é¢˜çš„åœ°å¸‚åˆ†ææŠ¥å‘Šã€‚

ã€åŸå§‹åˆ†ææŠ¥å‘Šã€‘ï¼ˆå­˜åœ¨é”™è¯¯ï¼Œéœ€è¦ä¿®æ­£ï¼‰ï¼š
{original_analysis}

ã€å‘ç°çš„é—®é¢˜ã€‘ï¼š
{errors_text}

ã€åŸå§‹æ•°æ®ç±»å‹ã€‘ï¼šåœ°å¸‚æ•°æ®
ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{user_input}
ã€åŸå§‹æ•°æ®ã€‘ï¼š{chart_obj}

ã€å‡†ç¡®çš„æ’åºç»“æœã€‘ï¼š
{sort_results}

ã€å‡†ç¡®ç»Ÿè®¡ä¿¡æ¯ã€‘ï¼š
{stats_summary}

ã€ä¿®æ­£è¦æ±‚ã€‘ï¼š
1. **æ•°æ®å‡†ç¡®æ€§**ï¼šæ‰€æœ‰æ’åã€æ•°å€¼å¿…é¡»ä¸¥æ ¼åŸºäºæ’åºç»“æœï¼Œä¸¥ç¦ç¼–é€ 
2. **åœ°å¸‚åç§°**ï¼šå¿…é¡»ä½¿ç”¨å…·ä½“åœ°å¸‚åç§°ï¼Œé¿å…"æœ€é«˜åœ°å¸‚"ç­‰æ³›åŒ–è¡¨è¿°
3. **æ’åºå‡†ç¡®æ€§**ï¼šç¡®ä¿"æœ€é«˜"ã€"æœ€ä½"ç­‰è¡¨è¿°ä¸å®é™…æ’åºä¸€è‡´
4. **ç»Ÿè®¡å‡†ç¡®æ€§**ï¼šå‡å€¼ã€ç»Ÿè®¡æ•°é‡å¿…é¡»ä½¿ç”¨æä¾›çš„ç»Ÿè®¡ä¿¡æ¯
5. **é€»è¾‘ä¸€è‡´æ€§**ï¼šåŒä¸€åœ°å¸‚ä¸èƒ½æ—¢æ˜¯æœ€é«˜åˆæ˜¯æœ€ä½
6. **ç¦æ­¢äº‹é¡¹**ï¼šä¸¥ç¦å°†å„åœ°å¸‚å æ¯”ç›¸åŠ ï¼Œä¸¥ç¦ç¼–é€ ä¸å­˜åœ¨çš„æ•°æ®

è¯·åŸºäºä¸Šè¿°è¦æ±‚ï¼Œé‡æ–°ç”Ÿæˆå‡†ç¡®çš„åœ°å¸‚åˆ†ææŠ¥å‘Šï¼š"""
    
    def _build_general_correction_prompt(self, original_analysis: str, user_input: str, 
                                       chart_obj: str, sort_results: str, statistics: Dict, 
                                       errors_text: str) -> str:
        """æ„å»ºé€šç”¨çš„ä¿®æ­£æç¤ºè¯"""
        stats_summary = self._build_statistics_summary(statistics)
        
        return f"""ä½ æ˜¯ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œéœ€è¦ä¿®æ­£ä¸€ä»½æœ‰é—®é¢˜çš„åˆ†ææŠ¥å‘Šã€‚

ã€åŸå§‹åˆ†ææŠ¥å‘Šã€‘ï¼ˆå­˜åœ¨é”™è¯¯ï¼Œéœ€è¦ä¿®æ­£ï¼‰ï¼š
{original_analysis}

ã€å‘ç°çš„é—®é¢˜ã€‘ï¼š
{errors_text}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{user_input}
ã€åŸå§‹æ•°æ®ã€‘ï¼š{chart_obj}

ã€å‡†ç¡®çš„æ’åºç»“æœã€‘ï¼š
{sort_results}

ã€å‡†ç¡®ç»Ÿè®¡ä¿¡æ¯ã€‘ï¼š
{stats_summary}

ã€ä¿®æ­£è¦æ±‚ã€‘ï¼š
1. åŸºäºå®é™…æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸¥ç¦ç¼–é€ ä»»ä½•ä¿¡æ¯
2. ç¡®ä¿æ‰€æœ‰æ•°å€¼æ¥æºäºæ’åºç»“æœæˆ–ç»Ÿè®¡ä¿¡æ¯
3. ä¿æŒé€»è¾‘ä¸€è‡´æ€§ï¼Œé¿å…è‡ªç›¸çŸ›ç›¾
4. è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé‡ç‚¹çªå‡º

è¯·åŸºäºä¸Šè¿°è¦æ±‚ï¼Œé‡æ–°ç”Ÿæˆå‡†ç¡®çš„åˆ†ææŠ¥å‘Šï¼š"""
    
    def _build_statistics_summary(self, statistics: Dict) -> str:
        """æ„å»ºç»Ÿè®¡ä¿¡æ¯æ‘˜è¦"""
        if not statistics:
            return "æ— ç»Ÿè®¡ä¿¡æ¯"
        
        summary_lines = []
        for indicator, stats in statistics.items():
            line = f"ã€{indicator}ã€‘å‡å€¼{stats['mean']}ï¼Œæœ€é«˜{stats['max']}ï¼Œæœ€ä½{stats['min']}ï¼Œ"
            
            if stats.get('is_time_series', False):
                line += f"é«˜äºå‡å€¼{stats['count_above_mean']}ä¸ªæ—¶é—´ç‚¹ï¼Œä½äºå‡å€¼{stats['count_below_mean']}ä¸ªæ—¶é—´ç‚¹"
            else:
                line += f"é«˜äºå‡å€¼{stats['count_above_mean']}ä¸ªåœ°å¸‚ï¼Œä½äºå‡å€¼{stats['count_below_mean']}ä¸ªåœ°å¸‚"
            
            summary_lines.append(line)
        
        return "\n".join(summary_lines)
    
    def _call_llm_for_correction(self, prompt: str) -> str:
        """è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œä¿®æ­£"""
        # å¤„ç†ä¸åŒAPIçš„URLæ ¼å¼
        if 'dashscope.aliyuncs.com' in self.base_url:
            # é˜¿é‡Œäº‘APIï¼Œbase_urlå·²ç»åŒ…å«äº†è·¯å¾„
            url = f"{self.base_url}/chat/completions"
        elif self.base_url.endswith('/v1'):
            # base_urlå·²ç»åŒ…å«/v1è·¯å¾„ï¼Œç›´æ¥æ·»åŠ endpoint
            url = f"{self.base_url}/chat/completions"
        else:
            # éœ€è¦æ·»åŠ å®Œæ•´è·¯å¾„
            url = f"{self.base_url}/v1/chat/completions"
            
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model_name,
            "messages": [
                {
                    "role": "system", 
                    "content": "ä½ æ˜¯ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆå’Œäº‹å®æ£€æŸ¥å‘˜ã€‚ä¸¥æ ¼è¦æ±‚ï¼š1)æ‰€æœ‰åˆ†æå¿…é¡»åŸºäºæä¾›çš„æ•°æ® 2)ç¦æ­¢ç¼–é€ ä»»ä½•ä¿¡æ¯ 3)ç¡®ä¿é€»è¾‘ä¸€è‡´æ€§ 4)æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©åˆé€‚çš„åˆ†æè§’åº¦"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "temperature": 0.1  # é™ä½æ¸©åº¦ä»¥æé«˜å‡†ç¡®æ€§
        }
        
        # å¦‚æœæ˜¯Qwen3æ¨¡å‹ï¼Œæ·»åŠ enable_thinkingå‚æ•°
        if 'enable_thinking' in MODEL_CONFIG:
            payload['extra_body'] = {"enable_thinking": MODEL_CONFIG['enable_thinking']}
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=self.timeout)
            response.raise_for_status()
            result = response.json()
            
            if "choices" in result and result["choices"]:
                content = result["choices"][0]["message"]["content"]
                if content and content.strip():
                    print("âœ… åˆ†æç»“è®ºä¿®æ­£å®Œæˆ")
                    return content.strip()
            
            print("âŒ ä¿®æ­£å¤±è´¥ï¼šAPIè¿”å›å†…å®¹ä¸ºç©º")
            return "ä¿®æ­£å¤±è´¥ï¼Œè¯·äººå·¥æ£€æŸ¥"
            
        except Exception as e:
            print(f"âŒ ä¿®æ­£è¿‡ç¨‹å‡ºé”™: {e}")
            return "ä¿®æ­£å¤±è´¥ï¼Œè¯·äººå·¥æ£€æŸ¥"


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    verifier = AnalysisVerifier()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_analysis = """ã€æ‹ç…§å…¨çƒé€šä¿æœ‰ç‡åˆ†æã€‘  
ç»¼åˆä¿æœ‰ç‡è¡¨ç°çªå‡ºï¼šé”¦å·å¸‚ä»¥99.2%çš„ç»¼åˆä¿æœ‰ç‡ä½å±…å…¨çœç¬¬ä¸€ï¼Œé«˜äºå‡å€¼1.0ä¸ªç™¾åˆ†ç‚¹ï¼Œæ˜¯å”¯ä¸€çªç ´99%çš„åœ°å¸‚ã€‚  
ä»·å€¼ä¿æœ‰ç‡å·®è·æ˜¾è‘—ï¼šä¸¹ä¸œå¸‚ä»·å€¼ä¿æœ‰ç‡95.39%ä¸ºå…¨çœæœ€ä½ï¼Œä½äºå‡å€¼2.01ä¸ªç™¾åˆ†ç‚¹ï¼Œéœ€é‡ç‚¹æå‡é«˜ä»·å€¼å®¢æˆ·ç•™å­˜èƒ½åŠ›ã€‚"""
    
    test_data = """æ•°æ®åˆ†æè¡¨æ ¼

æœˆä»½	æ‹ç…§å…¨çƒé€šç»¼åˆä¿æœ‰ç‡(%)	æ‹ç…§å…¨çƒé€šè§„æ¨¡ä¿æœ‰ç‡(%)	æ‹ç…§å…¨çƒé€šä»·å€¼ä¿æœ‰ç‡(%)
202501	98.20	100	96.40
202502	99.20	99.80	98.60
202503	99.20	99.50	98.90
202504	98.50	99.10	97.80
202505	97.80	98.80	96.80
202506	96.95	98.50	95.39
202507	98.98	98.18	99.77
202508	96.8	97.9	95.8

æ•°æ®è¯´æ˜ï¼šå…±8è¡Œæ•°æ®ï¼Œ4ä¸ªæŒ‡æ ‡"""
    
    test_user_input = 'è¯·åˆ†æå…¨çƒé€š"é‡è´¨æ„æ•ˆ"åˆ†æï¼Œåˆ†ææœˆä»½ï¼š202508'
    test_task_info = {"anaylsis_name": "å…¨çƒé€šé‡è´¨æ„æ•ˆåˆ†æ", "op_month": "202508"}
    
    corrected_analysis, errors = verifier.verify_and_correct_analysis(
        test_analysis, test_user_input, test_data, test_task_info
    )
    
    print("ä¿®æ­£åçš„åˆ†æï¼š")
    print(corrected_analysis)
    print("\nå‘ç°çš„é—®é¢˜ï¼š")
    for error in errors:
        print(f"- {error['description']}")
